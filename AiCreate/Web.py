import streamlit as st
import av
import cv2
import numpy as np
import os
import gdown
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration

<<<<<<< HEAD
<<<<<<< HEAD
def show(video_feed_url):
    st.title("üì∑ ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏•‡πÑ‡∏ó‡∏°‡πå")

    if video_feed_url:
        st.image(video_feed_url)  # ‚úÖ ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏à‡∏≤‡∏Å Flask `/video_feed`
    else:
        st.error("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏à‡∏≤‡∏Å Flask ‡πÑ‡∏î‡πâ")
=======
def show():
    # üîπ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Google Drive File ID ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
    GDRIVE_FILE_ID = "1d2UdtGOP-R0Hdg3vatxTWVH2tNyYtfo9"
    MODEL_PATH = "NNmodel.h5"

=======
def show():
    # üîπ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Google Drive File ID ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
    GDRIVE_FILE_ID = "1d2UdtGOP-R0Hdg3vatxTWVH2tNyYtfo9"
    MODEL_PATH = "NNmodel.h5"

>>>>>>> parent of d1bccf4 (Update camera)
    # üîπ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÉ‡∏´‡πâ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å Google Drive
    if not os.path.exists(MODEL_PATH):
        st.info("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å Google Drive...")
        gdown.download(f"https://drive.google.com/uc?id={GDRIVE_FILE_ID}", MODEL_PATH, quiet=False)

    # üîπ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
    model = load_model(MODEL_PATH)
    st.success("‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")

    # üîπ Map label index ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™
    class_labels = {0: "Angry", 1: "Happy", 2: "Normal", 3: "Sleep"}

    # üîπ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ WebRTC Signaling Server
    rtc_config = RTCConfiguration(
        {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
    )

    # üîπ ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Streamlit WebRTC
    class EmotionDetector(VideoTransformerBase):
        def __init__(self):
            self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

        def transform(self, frame: av.VideoFrame) -> np.ndarray:
            img = frame.to_ndarray(format="bgr24")
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(50, 50))

            for (x, y, w, h) in faces:
                face = img[y:y+h, x:x+w]
                face = cv2.resize(face, (200, 200))
                face_array = image.img_to_array(face) / 255.0
                face_array = np.expand_dims(face_array, axis=0)
                
                prediction = model.predict(face_array)
                predicted_class = np.argmax(prediction)
                predicted_label = class_labels[predicted_class]
                
                cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)
                cv2.putText(img, predicted_label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
            
            return img

    # üîπ ‡∏™‡πà‡∏ß‡∏ô UI ‡∏Ç‡∏≠‡∏á Streamlit
    st.title("‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏à‡∏≤‡∏Å‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏•‡πÑ‡∏ó‡∏°‡πå")
    st.write("‡πÄ‡∏õ‡∏¥‡∏î‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏•‡∏≤‡∏á‡πÄ‡∏ü‡∏£‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå!")

<<<<<<< HEAD
    webrtc_streamer(key="emotion-detection", video_transformer_factory=EmotionDetector, rtc_configuration=rtc_config)
>>>>>>> parent of 96cd212 (Revert "Update Web.py")
=======
    webrtc_streamer(key="emotion-detection", video_transformer_factory=EmotionDetector, rtc_configuration=rtc_config)
>>>>>>> parent of d1bccf4 (Update camera)
