import streamlit as st

<<<<<<< HEAD
def show(video_feed_url):
    st.title("ðŸ“· à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¸­à¸²à¸£à¸¡à¸“à¹Œà¹à¸šà¸šà¹€à¸£à¸µà¸¢à¸¥à¹„à¸—à¸¡à¹Œ")

    if video_feed_url:
        st.image(video_feed_url)  # âœ… à¹à¸ªà¸”à¸‡à¸§à¸´à¸”à¸µà¹‚à¸­à¸ˆà¸²à¸ Flask `/video_feed`
    else:
        st.error("âš ï¸ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹‚à¸«à¸¥à¸”à¸§à¸´à¸”à¸µà¹‚à¸­à¸ˆà¸²à¸ Flask à¹„à¸”à¹‰")
=======
def show():
    # ðŸ”¹ à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² Google Drive File ID à¸‚à¸­à¸‡à¹‚à¸¡à¹€à¸”à¸¥
    GDRIVE_FILE_ID = "1d2UdtGOP-R0Hdg3vatxTWVH2tNyYtfo9"
    MODEL_PATH = "NNmodel.h5"

    # ðŸ”¹ à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¹„à¸Ÿà¸¥à¹Œà¹‚à¸¡à¹€à¸”à¸¥ à¹ƒà¸«à¹‰à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”à¸ˆà¸²à¸ Google Drive
    if not os.path.exists(MODEL_PATH):
        st.info("à¸à¸³à¸¥à¸±à¸‡à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥à¸ˆà¸²à¸ Google Drive...")
        gdown.download(f"https://drive.google.com/uc?id={GDRIVE_FILE_ID}", MODEL_PATH, quiet=False)

    # ðŸ”¹ à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥
    model = load_model(MODEL_PATH)
    st.success("à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥à¸ªà¸³à¹€à¸£à¹‡à¸ˆ!")

    # ðŸ”¹ Map label index à¹€à¸›à¹‡à¸™à¸Šà¸·à¹ˆà¸­à¸„à¸¥à¸²à¸ª
    class_labels = {0: "Angry", 1: "Happy", 2: "Normal", 3: "Sleep"}

    # ðŸ”¹ à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² WebRTC Signaling Server
    rtc_config = RTCConfiguration(
        {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
    )

    # ðŸ”¹ à¸•à¸±à¸§à¹à¸›à¸¥à¸‡à¸§à¸´à¸”à¸µà¹‚à¸­à¸ªà¸³à¸«à¸£à¸±à¸š Streamlit WebRTC
    class EmotionDetector(VideoTransformerBase):
        def __init__(self):
            self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

        def transform(self, frame: av.VideoFrame) -> np.ndarray:
            img = frame.to_ndarray(format="bgr24")
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(50, 50))

            for (x, y, w, h) in faces:
                face = img[y:y+h, x:x+w]
                face = cv2.resize(face, (200, 200))
                face_array = image.img_to_array(face) / 255.0
                face_array = np.expand_dims(face_array, axis=0)
                
                prediction = model.predict(face_array)
                predicted_class = np.argmax(prediction)
                predicted_label = class_labels[predicted_class]
                
                cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)
                cv2.putText(img, predicted_label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
            
            return img

    # ðŸ”¹ à¸ªà¹ˆà¸§à¸™ UI à¸‚à¸­à¸‡ Streamlit
    st.title("à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¸­à¸²à¸£à¸¡à¸“à¹Œà¸ˆà¸²à¸à¹ƒà¸šà¸«à¸™à¹‰à¸²à¹à¸šà¸šà¹€à¸£à¸µà¸¢à¸¥à¹„à¸—à¸¡à¹Œ")
    st.write("à¹€à¸›à¸´à¸”à¸à¸¥à¹‰à¸­à¸‡à¹à¸¥à¸°à¹ƒà¸«à¹‰à¹ƒà¸šà¸«à¸™à¹‰à¸²à¸­à¸¢à¸¹à¹ˆà¸à¸¥à¸²à¸‡à¹€à¸Ÿà¸£à¸¡à¹€à¸žà¸·à¹ˆà¸­à¸”à¸¹à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œ!")

    webrtc_streamer(key="emotion-detection", video_transformer_factory=EmotionDetector, rtc_configuration=rtc_config)
>>>>>>> parent of 96cd212 (Revert "Update Web.py")
